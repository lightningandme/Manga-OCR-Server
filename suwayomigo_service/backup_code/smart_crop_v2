import base64
import os
import sys
import cv2
import numpy as np
import uvicorn
from PIL import Image
from fastapi import FastAPI, Body
from dotenv import load_dotenv
import torch
import easyocr
import time
import warnings

# å±è”½æ‰æ¥è‡ª huggingface_hub çš„ FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning, module="huggingface_hub")
# å…³é”®ï¼šå°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from manga_ocr import MangaOcr
from janome.tokenizer import Tokenizer
from openai import OpenAI

# åŠ è½½å½“å‰ç›®å½•ä¸‹çš„ .env æ–‡ä»¶
load_dotenv()

# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–
api_key = os.getenv("DEEPSEEK_API_KEY")
base_url = os.getenv("DEEPSEEK_BASE_URL")
# --- é…ç½® DeepSeek ---
client = OpenAI(
    api_key=api_key,
    base_url=base_url
)

app = FastAPI()

# 1. åˆå§‹åŒ–æ£€æµ‹å™¨ (åªå¼€å¯æ£€æµ‹åŠŸèƒ½ï¼Œä¸å¼€å¯è¯†åˆ«ï¼Œé€Ÿåº¦æå¿«)
print("Initializing CRAFT Text Detector...")
gpu_available = torch.cuda.is_available()
reader = easyocr.Reader(['ja'], gpu=gpu_available)

# åˆå§‹åŒ– OCR æ¨¡å‹
print("Loading Manga-OCR model...")
mocr = MangaOcr()

# åˆå§‹åŒ– Janome åˆ†è¯å™¨ (æœ¬åœ°è¿è¡Œï¼Œæå¿«)
tokenizer = Tokenizer()


def is_blocked_by_line(img_gray, pt1, pt2):
    """
    æ¢æµ‹ä¸¤ç‚¹ä¹‹é—´æ˜¯å¦æœ‰æ˜æ˜¾çš„çº¿æ¡é˜»éš”
    """
    # 1. å±€éƒ¨äºŒå€¼åŒ–ï¼Œè·å–è¾¹ç¼˜çº¿æ¡
    # ä½¿ç”¨ Canny æå–æ˜æ˜¾çš„çº¿æ¡ï¼ˆåˆ†é•œçº¿æˆ–å¯¹è¯æ¡†çº¿ï¼‰
    edges = cv2.Canny(img_gray, 50, 150)

    # 2. åœ¨ä¸¤ç‚¹é—´çº¿æ€§é‡‡æ ·
    num_samples = 10
    points_x = np.linspace(pt1[0], pt2[0], num_samples).astype(int)
    points_y = np.linspace(pt1[1], pt2[1], num_samples).astype(int)

    hits = 0
    h, w = edges.shape
    for i in range(num_samples):
        px, py = points_x[i], points_y[i]
        if 0 <= px < w and 0 <= py < h:
            # æ£€æŸ¥é‡‡æ ·ç‚¹åŠå…¶å‘¨å›´æå°èŒƒå›´æ˜¯å¦æœ‰å¼ºè¾¹ç¼˜
            if np.any(edges[max(0, py - 1):min(h, py + 1), max(0, px - 1):min(w, px + 1)] > 0):
                hits += 1

    # å¦‚æœæœ‰è¶…è¿‡ 2 ä¸ªç‚¹æ’å¢™ï¼Œè®¤ä¸ºå­˜åœ¨ç‰©ç†é˜»éš”
    return hits >= 2


def get_smart_crop(image_bytes, click_x, click_y):
    nparr = np.frombuffer(image_bytes, np.uint8)
    img_cv2 = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if img_cv2 is None: return None

    h_img, w_img = img_cv2.shape[:2]
    gray = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2GRAY)
    debug_vis = img_cv2.copy()

    # 1. è¯­ä¹‰æ£€æµ‹ (EasyOCR)
    horizontal_list, _ = reader.detect(img_cv2, text_threshold=0.5, link_threshold=0.4, low_text=0.3)
    raw_boxes = horizontal_list[0] if horizontal_list else []

    if not raw_boxes:
        return fallback_crop(img_cv2, click_x, click_y, "No Boxes Found", debug_vis)

    # === è°ƒè¯•ä»£ç ï¼šç»˜åˆ¶åŸå§‹ç«æŸ´ç›’ ===
    debug_boxes_img = img_cv2.copy()
    for i, b in enumerate(raw_boxes):
        # b çš„æ ¼å¼å–å†³äºä½ ä½¿ç”¨çš„æ£€æµ‹å™¨ï¼ŒEasyOCR detect() é€šå¸¸è¿”å› [xmin, xmax, ymin, ymax]
        bx1, bx2, by1, by2 = map(int, b)
        # ç”»å‡ºåŸå§‹ç«æŸ´ç›’ï¼ˆç»¿è‰²ï¼‰
        cv2.rectangle(debug_boxes_img, (bx1, by1), (bx2, by2), (0, 255, 0), 2)
        # æ ‡æ³¨åºå·ï¼Œæ–¹ä¾¿è¿½è¸ª
        cv2.putText(debug_boxes_img, str(i), (bx1, by1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)

    # ç”»å‡ºç‚¹å‡»ç‚¹ï¼ˆçº¢è‰²ï¼‰
    cv2.circle(debug_boxes_img, (click_x, click_y), 5, (0, 0, 255), -1)
    # ä¿å­˜è°ƒè¯•å›¾
    cv2.imwrite("debug_raw_fireboxes.png", debug_boxes_img)
    print(f"ğŸ–¼ï¸ åŸå§‹ç«æŸ´ç›’è°ƒè¯•å›¾å·²ä¿å­˜è‡³: debug_raw_fireboxes.png (å…± {len(raw_boxes)} ä¸ª)")
    # ============================
    # 2. èšç±»é€»è¾‘ (V8.8 ç‰©ç†éš”ç¦»ç‰ˆ)
    grouped_boxes = []
    used = [False] * len(raw_boxes)
    avg_line_h = np.mean([b[3] - b[2] for b in raw_boxes]) if raw_boxes else 30

    for i in range(len(raw_boxes)):
        if used[i]: continue
        current_cluster = [raw_boxes[i]]
        used[i] = True

        found_new = True
        while found_new:
            found_new = False
            c_x1 = min(b[0] for b in current_cluster)
            c_x2 = max(b[1] for b in current_cluster)
            c_y1 = min(b[2] for b in current_cluster)
            c_y2 = max(b[3] for b in current_cluster)

            center_curr = ((c_x1 + c_x2) / 2, (c_y1 + c_y2) / 2)

            for j in range(len(raw_boxes)):
                if used[j]: continue
                bx1, bx2, by1, by2 = raw_boxes[j]
                center_next = ((bx1 + bx2) / 2, (by1 + by2) / 2)

                dx = max(0, c_x1 - bx2, bx1 - c_x2)
                dy = max(0, c_y1 - by2, by1 - c_y2)

                # åˆ¤å®šæ¡ä»¶ 1: åŸºç¡€å‡ ä½•è·ç¦»
                if dy < avg_line_h * 1.2 and dx < avg_line_h * 0.6:

                    # --- V8.8 æ ¸å¿ƒé€»è¾‘ï¼šåˆ†ç•Œçº¿ç©¿é€æ£€æµ‹ ---
                    if is_blocked_by_line(gray, center_curr, center_next):
                        # print(f"ğŸš« æ‹¦æˆªï¼šæ¢æµ‹åˆ°ç‰©ç†çº¿æ¡é˜»éš”ï¼Œæ‹’ç»åˆå¹¶ B{i} å’Œ B{j}")
                        continue

                    current_cluster.append(raw_boxes[j])
                    used[j] = True
                    found_new = True

        grouped_boxes.append((min(b[0] for b in current_cluster),
                              max(b[1] for b in current_cluster),
                              min(b[2] for b in current_cluster),
                              max(b[3] for b in current_cluster)))

    print(f"ğŸ“¦ [Step 2] èšç±»å®Œæˆï¼Œéš”ç¦»ååˆå¹¶ä¸º {len(grouped_boxes)} ä¸ªæ–‡æœ¬å—")

    # --- 3. åŒ¹é…ä¸è°ƒè¯•å›¾ç‰‡ (åŒå‰) ---
    best_box = None
    min_dist = float('inf')
    slack = 15  # ç¼©å°ç‚¹å‡»æ„Ÿåº”åŒºï¼Œæé«˜ç²¾å‡†åº¦

    for idx, box in enumerate(grouped_boxes):
        x1, x2, y1, y2 = map(int, box)
        cv2.rectangle(debug_vis, (x1, y1), (x2, y2), (255, 100, 0), 2)

        # å‘½ä¸­åˆ¤å®šï¼šä¼˜å…ˆåˆ¤å®šç‚¹å‡»ç‚¹æ˜¯å¦åœ¨å†…éƒ¨
        if (x1 - slack) <= click_x <= (x2 + slack) and (y1 - slack) <= click_y <= (y2 + slack):
            cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
            dist = ((click_x - cx) ** 2 + (click_y - cy) ** 2) ** 0.5
            if dist < min_dist:
                min_dist = dist
                best_box = (x1, x2, y1, y2)

    # å¦‚æœæ²¡å‘½ä¸­ï¼Œå¯»æ‰¾å…¨å±€æœ€è¿‘
    if best_box is None:
        print("ğŸ” æœªç›´æ¥å‘½ä¸­ï¼Œå¯»æ‰¾æœ€è¿‘æ–‡æœ¬å—...")
        for box in grouped_boxes:
            x1, x2, y1, y2 = map(int, box)
            cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
            dist = ((click_x - cx) ** 2 + (click_y - cy) ** 2) ** 0.5
            if dist < min_dist:
                min_dist = dist
                best_box = (x1, x2, y1, y2)

    # --- 4. æœ€ç»ˆåˆ¤å®š ---
    if best_box:
        # è·ç¦»é™åˆ¶æ”¹ä¸ºæ›´åˆç†çš„ 300px (æˆ–è€…çŸ­è¾¹çš„ 1/4)
        max_limit = min(h_img, w_img) / 3
        if min_dist > 0 and min_dist > max_limit:
            return fallback_crop(img_cv2, click_x, click_y, "Too Far From Blocks", debug_vis)

        x1, x2, y1, y2 = best_box
        # ä¼˜åŒ– Paddingï¼Œä¸è¦å¤ªå¤§
        pw, ph = int((x2 - x1) * 0.1) + 10, int((y2 - y1) * 0.05) + 10
        cx1, cy1 = max(0, x1 - pw), max(0, y1 - ph)
        cx2, cy2 = min(w_img, x2 + pw), min(h_img, y2 + ph)

        # æœ€ç»ˆè°ƒè¯•ç»˜å›¾
        cv2.rectangle(debug_vis, (cx1, cy1), (cx2, cy2), (0, 255, 255), 2)  # é»„è‰²è£å‰ªæ¡†
        cv2.circle(debug_vis, (click_x, click_y), 6, (0, 0, 255), -1)  # çº¢è‰²ç‚¹å‡»ç‚¹
        cv2.imwrite("debug_easyocr.png", debug_vis)

        return img_cv2[cy1:cy2, cx1:cx2]

    return fallback_crop(img_cv2, click_x, click_y, "End of Logic", debug_vis)


def fallback_crop(img, cx, cy, reason, debug_vis=None):
    print(f"ğŸ©¹ [Fallback] åŸå› : {reason}")
    h, w = img.shape[:2]
    d = 200
    x1, y1 = max(0, cx - d), max(0, cy - d)
    x2, y2 = min(w, cx + d), min(h, cy + d)

    if debug_vis is not None:
        cv2.rectangle(debug_vis, (x1, y1), (x2, y2), (0, 0, 255), 3)  # ä¿åº•æ¡†ç”¨çº¢è‰²
        cv2.circle(debug_vis, (cx, cy), 7, (0, 0, 255), -1)
        cv2.putText(debug_vis, f"Fallback: {reason}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
        cv2.imwrite("debug_easyocr.png", debug_vis)

    return img[y1:y2, x1:x2]

def analyze_text(text: str):
    """
    å¯¹æ—¥è¯­æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶æå–å­¦ä¹ ç›¸å…³çš„å…ƒæ•°æ®
    """
    results = []
    # è¿è¡Œåˆ†è¯
    tokens = tokenizer.tokenize(text)

    for token in tokens:
        # pos æ ¼å¼é€šå¸¸ä¸º: åè©,ä¸€èˆ¬,*,*
        pos_details = token.part_of_speech.split(',')
        main_pos = pos_details[0]  # ä¸»è¯æ€§

        # è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·å’Œç©ºç™½ï¼Œåªä¿ç•™æœ‰æ„ä¹‰çš„è¯æ±‡
        if main_pos in ['è¨˜å·', 'åŠ©è©', 'åŠ©å‹•è©'] and token.surface in [' ', 'ã€€', 'ã€‚', 'ã€','ï¼','ï¼','ï¼Ÿ','ï¼š']:
            continue

        results.append({
            "s": token.surface,  # è¡¨é¢å½¢ï¼šæ¼«ç”»é‡Œæ˜¾ç¤ºçš„åŸæ–‡
            "b": token.base_form,  # åŸå‹ï¼šæŸ¥è¯å…¸ç”¨çš„åŸå§‹å½¢æ€
            "p": main_pos,  # è¯æ€§ï¼šåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰
            "r": token.reading  # è¯»éŸ³ï¼šç‰‡å‡åè¯»éŸ³ (å¯é€‰)
        })
    return results


def get_ai_translation(text: str, manga_name: str):
    manga, episode = manga_name.rsplit(':', 1) if ':' in manga_name else ("æ—¥æœ¬æ¼«ç”»","æŸä¸€è¯")
    if not text.strip():
        return ""

    try:
        start_time = time.time()
        # ä½¿ç”¨æç®€ Promptï¼šä¸è¦æ±‚è§£é‡Šï¼Œåªè¦æ±‚åœ°é“ç¿»è¯‘å’Œæ ¸å¿ƒè¯åŸå‹
        # noinspection PyTypeChecker

        # æŒ‰ç…§ä½ æä¾›çš„ Prompt æ¨¡æ¿æ„å»º System Content
        system_content = (
            f"ä½ æ˜¯ä¸€ä½ç²¾é€šå¤šé—¨è¯­è¨€çš„æ—¥æœ¬æ¼«ç”»ç¿»è¯‘ä¸“å®¶ï¼Œæ­£åœ¨é˜…è¯»ã€Š{manga}ã€‹çš„{episode}ã€‚ \n"
            "ä½ çš„ä»»åŠ¡æ˜¯å¤„ç†æ¥è‡ª OCR è¯†åˆ«çš„åŸæ–‡ï¼Œå¹¶å®Œæˆä»¥ä¸‹ä¸‰æ­¥ï¼š\n"
            "1. **æ–‡æœ¬æ ¡å¯¹**ï¼šåˆ¤æ–­è¯†åˆ«ç»“æœä¸­æ˜¯å¦å­˜åœ¨å› ç¬”ç”»å¯†é›†å¯¼è‡´çš„é”™åˆ«å­—ï¼Œè¯·ç»“åˆè¯­å¢ƒå°†å…¶ä¿®æ­£ï¼ˆä¾‹å¦‚å°†é”™è¯¯çš„å½¢è¿‘å­—è¿˜åŸä¸ºæ­£ç¡®çš„è¯æ±‡ï¼‰ã€‚\n"
            "2. **é€»è¾‘æ–­å¥**ï¼šåˆ¤æ–­å› æ¼«ç”»æ’ç‰ˆå¯¼è‡´çš„éæ­£å¸¸è¿å­—ï¼Œå¹¶è¿›è¡Œé€»è¾‘æ–­è¡Œæˆ–å¢åŠ æ ‡ç‚¹ï¼Œè¿˜åŸè§’è‰²çœŸå®çš„è¯´è¯èŠ‚å¥ã€‚\n"
            "3. **åœ°é“ç¿»è¯‘**ï¼šåŸºäºä¿®æ­£åçš„åŸæ–‡ï¼Œç»“åˆè¯¥ä½œå“åœ¨æ­¤é˜¶æ®µçš„å‰§æƒ…èƒŒæ™¯å’Œè§’è‰²èº«ä»½è¿›è¡Œç¿»è¯‘ã€‚\n\n"
            "è¯·ç¿»è¯‘æˆåœ°é“ã€æµç•…çš„ä¸­æ–‡ã€‚ç›´æ¥è¿”å›è¯‘æ–‡ã€‚"
        )

        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": text},
            ],
            stream=False,
            temperature=0.3,  # é™ä½éšæœºæ€§ï¼Œè®©ç¿»è¯‘æ›´ç¨³å®š
            max_tokens=150  # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œå‡å°‘ä¼ è¾“è€—æ—¶
        )
        duration = time.time() - start_time
        print(f"AIç¿»è¯‘ å“åº”è€—æ—¶: {duration:.2f}s (æ­£åœ¨çœ‹: ã€Š{manga}ã€‹çš„{episode})")
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ç¿»è¯‘å‡ºé”™äº†: {str(e)}"


# ç¼“å­˜æœ€è¿‘ä¸€æ¬¡çš„ OCR æ–‡æœ¬å’Œæ¼«ç”»å
last_ocr_text = ""
last_manga_name = "General"
@app.post("/ocr")
async def perform_ocr(payload: dict = Body(...)):
    global last_ocr_text, last_manga_name  # <--- ä¿®æ”¹è¿™é‡Œï¼ŒåŠ å…¥ last_manga_name
    last_ocr_text = ""  # æ¯æ¬¡è¯†åˆ«æ–°å›¾å‰å…ˆæ¸…ç©ºæ—§ç¼“å­˜
    img_b64 = payload.get("image")
    # è·å– Android ä¼ æ¥çš„ç‚¹å‡»åæ ‡
    click_x = payload.get("x", 0)
    click_y = payload.get("y", 0)
    manga_name = payload.get("mangaName", "General")

    last_manga_name = manga_name  # <--- æ ¸å¿ƒä¿®æ”¹ï¼šå°†æœ¬æ¬¡æ¼«ç”»åå­˜å…¥ç¼“å­˜
    if not img_b64:
        return {"status": "error", "message": "No image data"}

    try:
        # 1. è§£ç ä¸è¯†åˆ«
        img_data = base64.b64decode(img_b64)

        # --- æ™ºèƒ½åˆ‡å›¾æ ¸å¿ƒè°ƒç”¨ ---
        # æ³¨æ„ï¼šè¿™é‡Œçš„ img_data æ˜¯ Android ä¼ æ¥çš„ 400x400 æˆ– 600x600 çš„å±€éƒ¨å›¾
        # è¿™é‡Œçš„ click_x/y åº”è¯¥æ˜¯ç›¸å¯¹äºè¿™å¼ å±€éƒ¨å›¾çš„åæ ‡
        start_time = time.time()
        smart_img_mat = get_smart_crop(img_data, click_x, click_y)

        # å°† OpenCV çš„ Mat è½¬å› PIL Image ç»™ Manga-OCR ä½¿ç”¨
        smart_img_rgb = cv2.cvtColor(smart_img_mat, cv2.COLOR_BGR2RGB)
        image = Image.fromarray(smart_img_rgb)
        duration = time.time() - start_time
        print(f"å›¾ç‰‡æˆªå– å“åº”è€—æ—¶: {duration:.2f}s")

        # åç»­ OCR é€»è¾‘ä¸å˜
        start_time = time.time()
        text = mocr(image)
        last_ocr_text = text  # å­˜å…¥ç¼“å­˜

        words = analyze_text(text)
        duration = time.time() - start_time
        print(f"æ–‡æœ¬å¤„ç† å“åº”è€—æ—¶: {duration:.2f}s")

        # æ ¸å¿ƒï¼šè¿™é‡Œä¸å†è°ƒç”¨ AI ç¿»è¯‘ï¼Œç›´æ¥è¿”å›ï¼Œé€Ÿåº¦æå‡ 200%
        return {
            "status": "success",
            "text": text,
            "words": words,
            "translation": ""  # åˆå§‹ä¸ºç©º
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


@app.get("/get_translation")
async def get_translation():
    global last_ocr_text, last_manga_name  # <--- å£°æ˜è¯»å–è¿™ä¸¤ä¸ªå…¨å±€å˜é‡
    if not last_ocr_text:
        return {"translation": "æœªæ£€æµ‹åˆ°å¾…ç¿»è¯‘æ–‡å­—"}

    # è°ƒç”¨æ—¶ä¼ å…¥ç¼“å­˜çš„æ¼«ç”»å
    translation = get_ai_translation(last_ocr_text, last_manga_name)
    return {"translation": translation}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=12233)